{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZDU6MyLQw0CQ/N5io+wah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TesisDeepcodeUcuenca/Tesis-Deepcode-Ucuenca/blob/main/Codigos%20Convencionales/C%C3%B3digos_convencionales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfBVVkcoW7Qt"
      },
      "outputs": [],
      "source": [
        "#@title Convolucional 1/2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import commpy.channelcoding.convcode as cc\n",
        "import commpy.utilities as util\n",
        "import scipy.stats as stats  # Importar scipy.stats\n",
        "from google.colab import files\n",
        "\n",
        "# Define la función BSC (Binary Symmetric Channel)\n",
        "def bsc(input_bits, p_t):\n",
        "    \"\"\"\n",
        "    Binary Symmetric Channel.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_bits : 1D ndarray containing {0, 1}\n",
        "        Input array of bits to the channel.\n",
        "\n",
        "    p_t : float in [0, 1]\n",
        "        Transition/Error probability of the channel.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    output_bits : 1D ndarray containing {0, 1}\n",
        "        Output bits from the channel.\n",
        "    \"\"\"\n",
        "    output_bits = input_bits.copy()\n",
        "    flip_locs = (np.random.rand(len(output_bits)) <= p_t)\n",
        "    output_bits[flip_locs] = 1 ^ output_bits[flip_locs]\n",
        "    return output_bits\n",
        "\n",
        "# Define los parámetros del código convolucional\n",
        "memory = np.array(2, ndmin=1)\n",
        "g_matrix = np.array((0o5, 0o7), ndmin=2)  # Tasa de codificación 1/2\n",
        "trellis = cc.Trellis(memory, g_matrix)\n",
        "tb_depth = 5 * (memory[0] + 1)  # Profundidad de traceback recomendada\n",
        "\n",
        "# Probabilidades de error a probar\n",
        "error_probabilities = np.linspace(0.0001, 0.15, 20)\n",
        "\n",
        "# Número de tramas y longitud de cada mensaje\n",
        "num_block = 100000\n",
        "batch_size = 100\n",
        "num_messages_per_frame = 100\n",
        "message_length = 50\n",
        "num_epochs = 2\n",
        "\n",
        "# Almacena los BER promedio para cada probabilidad de error\n",
        "ber_results = {p_t: [] for p_t in error_probabilities}  # Diccionario para almacenar BER para cada probabilidad en cada época\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"\")\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(\" \")\n",
        "\n",
        "    for p_t in error_probabilities:\n",
        "        total_bit_errors = 0\n",
        "        total_bits = 0\n",
        "\n",
        "        print(f\"Simulating for error probability p_t={p_t}...\")\n",
        "\n",
        "        for _ in tqdm(range(int(num_block/batch_size))):\n",
        "            # Generar bits de mensaje aleatorios en un batch\n",
        "            message_bits_batch = np.random.randint(0, 2, (num_messages_per_frame, message_length))\n",
        "\n",
        "            # Codificar los bits del mensaje en un batch\n",
        "            coded_bits_batch = np.array([cc.conv_encode(message_bits, trellis) for message_bits in message_bits_batch])\n",
        "\n",
        "            # Pasar los bits codificados a través del canal BSC en un batch\n",
        "            received_bits_batch = np.array([bsc(coded_bits, p_t) for coded_bits in coded_bits_batch])\n",
        "\n",
        "            # Decodificar los bits recibidos en un batch\n",
        "            decoded_bits_batch = np.array([cc.viterbi_decode(received_bits.astype(float), trellis, tb_depth) for received_bits in received_bits_batch])\n",
        "\n",
        "            # Calcular el número de errores de bit en un batch\n",
        "            for message_bits, decoded_bits in zip(message_bits_batch, decoded_bits_batch):\n",
        "                num_bit_errors = util.hamming_dist(message_bits, decoded_bits[:len(message_bits)])\n",
        "                total_bit_errors += num_bit_errors\n",
        "                total_bits += len(message_bits)\n",
        "\n",
        "        # Calcular la BER promedio para esta época\n",
        "        ber = total_bit_errors / total_bits\n",
        "        ber_results[p_t].append(ber)\n",
        "        print(f\"BER for p_t={p_t}: {ber}\")\n",
        "\n",
        "# Convertir los resultados en un DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Epoch': np.repeat(range(1, num_epochs + 1), len(error_probabilities)),\n",
        "    'Probabilidad de Error (p_t)': np.tile(error_probabilities, num_epochs),\n",
        "    'BER': [ber for sublist in ber_results.values() for ber in sublist]\n",
        "})\n",
        "\n",
        "# Guardar los resultados en un archivo CSV\n",
        "csv_path = '/content/ber_vs_probabilidades.csv'\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "\n",
        "# Calcular el promedio y el intervalo de confianza\n",
        "mean_ber = []\n",
        "conf_intervals = []\n",
        "\n",
        "for p_t in error_probabilities:\n",
        "    data = ber_results[p_t]\n",
        "    mean = np.mean(data)\n",
        "    confidence_interval = stats.t.interval(0.95, len(data)-1, loc=mean, scale=stats.sem(data))\n",
        "\n",
        "    mean_ber.append(mean)\n",
        "    conf_intervals.append(confidence_interval)\n",
        "\n",
        "# Convertir listas a arrays para facilitar la manipulación\n",
        "mean_ber = np.array(mean_ber)\n",
        "conf_intervals = np.array(conf_intervals)\n",
        "\n",
        "# Graficar los resultados\n",
        "plt.figure()\n",
        "plt.errorbar(error_probabilities, mean_ber,\n",
        "             yerr=[mean_ber - conf_intervals[:, 0], conf_intervals[:, 1] - mean_ber],\n",
        "             fmt='o', capsize=5)\n",
        "plt.xlabel('Probabilidad de error (p_t)')\n",
        "plt.ylabel('Tasa de Error de Bit (BER)')\n",
        "plt.title('BER promedio vs Probabilidad de Error con intervalos de confianza')\n",
        "plt.grid(True)\n",
        "# plt.xscale('log')  # Escala logarítmica para el eje x si es necesario\n",
        "plt.yscale('log')  # Escala logarítmica para el eje y\n",
        "plt.show()\n",
        "\n",
        "# Descargar el archivo CSV\n",
        "files.download(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convolucional 1/3\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import commpy.channelcoding.convcode as cc\n",
        "import commpy.utilities as util\n",
        "import scipy.stats as stats  # Importar scipy.stats\n",
        "\n",
        "# Define la función BSC (Binary Symmetric Channel)\n",
        "def bsc(input_bits, p_t):\n",
        "    \"\"\"\n",
        "    Binary Symmetric Channel.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_bits : 1D ndarray containing {0, 1}\n",
        "        Input array of bits to the channel.\n",
        "\n",
        "    p_t : float in [0, 1]\n",
        "        Transition/Error probability of the channel.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    output_bits : 1D ndarray containing {0, 1}\n",
        "        Output bits from the channel.\n",
        "    \"\"\"\n",
        "    output_bits = input_bits.copy()\n",
        "    flip_locs = (np.random.rand(len(output_bits)) <= p_t)\n",
        "    output_bits[flip_locs] = 1 ^ output_bits[flip_locs]\n",
        "    return output_bits\n",
        "\n",
        "# Define los parámetros del código convolucional\n",
        "memory = np.array(3, ndmin=1)\n",
        "g_matrix = np.array((0o7, 0o7, 0o5), ndmin=2)  # Tasa de codificación 1/2\n",
        "trellis = cc.Trellis(memory, g_matrix)\n",
        "tb_depth = 5 * (memory[0] + 1)  # Profundidad de traceback recomendada\n",
        "\n",
        "# Probabilidades de error a probar\n",
        "#error_probabilities = np.linspace(0.0001, 0.15, 20)\n",
        "\n",
        "error_probabilities = np.array([0.134221053, 0.142110526, 0.15])\n",
        "\n",
        "\n",
        "\n",
        "# Número de tramas y longitud de cada mensaje\n",
        "num_block = 100000\n",
        "batch_size = 100\n",
        "num_messages_per_frame = 100\n",
        "message_length = 50\n",
        "num_epochs = 1\n",
        "\n",
        "# Almacena los BER promedio para cada probabilidad de error\n",
        "ber_results = {p_t: [] for p_t in error_probabilities}  # Diccionario para almacenar BER para cada probabilidad en cada época\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"\")\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(\" \")\n",
        "\n",
        "    for p_t in error_probabilities:\n",
        "        total_bit_errors = 0\n",
        "        total_bits = 0\n",
        "\n",
        "        print(f\"Simulating for error probability p_t={p_t}...\")\n",
        "\n",
        "        for _ in tqdm(range(int(num_block/batch_size))):\n",
        "            # Generar bits de mensaje aleatorios en un batch\n",
        "            message_bits_batch = np.random.randint(0, 2, (num_messages_per_frame, message_length))\n",
        "\n",
        "            # Codificar los bits del mensaje en un batch\n",
        "            coded_bits_batch = np.array([cc.conv_encode(message_bits, trellis) for message_bits in message_bits_batch])\n",
        "\n",
        "            #print(coded_bits_batch.shape)\n",
        "\n",
        "            # Pasar los bits codificados a través del canal BSC en un batch\n",
        "            received_bits_batch = np.array([bsc(coded_bits, p_t) for coded_bits in coded_bits_batch])\n",
        "            #print(received_bits_batch.shape)\n",
        "            # Decodificar los bits recibidos en un batch\n",
        "            decoded_bits_batch = np.array([cc.viterbi_decode(received_bits.astype(float), trellis, tb_depth) for received_bits in received_bits_batch])\n",
        "\n",
        "            # Calcular el número de errores de bit en un batch\n",
        "            for message_bits, decoded_bits in zip(message_bits_batch, decoded_bits_batch):\n",
        "                num_bit_errors = util.hamming_dist(message_bits, decoded_bits[:len(message_bits)])\n",
        "                total_bit_errors += num_bit_errors\n",
        "                total_bits += len(message_bits)\n",
        "\n",
        "        # Calcular la BER promedio para esta época\n",
        "        ber = total_bit_errors / total_bits\n",
        "        ber_results[p_t].append(ber)\n",
        "        print(f\"BER for p_t={p_t}: {ber}\")\n",
        "\n",
        "# Convertir los resultados en un DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Epoch': np.repeat(range(1, num_epochs + 1), len(error_probabilities)),\n",
        "    'Probabilidad de Error (p_t)': np.tile(error_probabilities, num_epochs),\n",
        "    'BER': [ber for sublist in ber_results.values() for ber in sublist]\n",
        "})\n",
        "\n",
        "# Guardar los resultados en un archivo CSV\n",
        "results_df.to_csv('ber_vs_probabilidades22.csv', index=False)\n",
        "\n",
        "# Calcular el promedio y el intervalo de confianza\n",
        "mean_ber = []\n",
        "conf_intervals = []\n",
        "\n",
        "for p_t in error_probabilities:\n",
        "    data = ber_results[p_t]\n",
        "    mean = np.mean(data)\n",
        "    confidence_interval = stats.t.interval(0.95, len(data)-1, loc=mean, scale=stats.sem(data))\n",
        "\n",
        "    mean_ber.append(mean)\n",
        "    conf_intervals.append(confidence_interval)\n",
        "\n",
        "# Convertir listas a arrays para facilitar la manipulación\n",
        "mean_ber = np.array(mean_ber)\n",
        "conf_intervals = np.array(conf_intervals)\n",
        "\n",
        "# Graficar los resultados\n",
        "plt.figure()\n",
        "plt.errorbar(error_probabilities, mean_ber,\n",
        "             yerr=[mean_ber - conf_intervals[:, 0], conf_intervals[:, 1] - mean_ber],\n",
        "             fmt='o', capsize=5)\n",
        "plt.xlabel('Probabilidad de error (p_t)')\n",
        "plt.ylabel('Tasa de Error de Bit (BER)')\n",
        "plt.title('BER promedio vs Probabilidad de Error con intervalos de confianza')\n",
        "plt.grid(True)\n",
        "# plt.xscale('log')  # Escala logarítmica para el eje x si es necesario\n",
        "plt.yscale('log')  # Escala logarítmica para el eje y\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EtBedQD8W_r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Codigos Hamming (15, 11) y datos crudos\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "def guardar_matriz_en_csv(matriz, nombre_archivo):\n",
        "    # Convertir la matriz a un DataFrame de pandas\n",
        "    df = pd.DataFrame(matriz.T)  # Transponer para que cada columna corresponda a una probabilidad\n",
        "\n",
        "    # Asignar nombres a las columnas como Probabilidad 1, Probabilidad 2, etc.\n",
        "    df.columns = [f'Probabilidad {i + 1}' for i in range(matriz.shape[0])]\n",
        "\n",
        "    # Guardar el DataFrame en un archivo CSV\n",
        "    df.to_csv(nombre_archivo, index=False)\n",
        "def canal_bsc(mensaje, ruido):\n",
        "    channel_message = np.bitwise_xor(mensaje, ruido)\n",
        "    return channel_message\n",
        "def hamming_data(bits):\n",
        "    matrix_dimension = int(np.ceil(np.log2(len(bits))))\n",
        "    potencias_de_dos = [2 ** i for i in range(matrix_dimension + 1)]\n",
        "    posiciones_info = [i for i in range(1, 2 ** matrix_dimension + 1) if i not in potencias_de_dos]\n",
        "    return posiciones_info, matrix_dimension\n",
        "def calculate_ber(original_bits, decoded_bits):\n",
        "    error_count = np.sum(original_bits != decoded_bits)\n",
        "    ber = error_count / len(original_bits)\n",
        "    return ber\n",
        "def hamming_decode(Rx_bits, posiciones_info):\n",
        "    uncoded_bits = []\n",
        "    parity_check = 0\n",
        "    for i, bit in enumerate(Rx_bits):\n",
        "        if bit:\n",
        "            parity_check ^= i\n",
        "    if parity_check != 0:\n",
        "        Rx_bits[parity_check] ^= 1\n",
        "    for pos in posiciones_info:\n",
        "        uncoded_bits.append(Rx_bits[pos])\n",
        "\n",
        "    return uncoded_bits\n",
        "def hamming_encode(bits, posiciones_info, matrix_dimension):\n",
        "    bits_data = list(enumerate(bits))\n",
        "    matriz_hamming = np.zeros(2**matrix_dimension, dtype=int)\n",
        "    bit_activados = np.zeros((matrix_dimension, ((2*matrix_dimension)-1)), dtype=int)\n",
        "    flags_paridad = np.zeros(matrix_dimension, dtype=int)\n",
        "    bits_paridad = np.zeros(matrix_dimension, dtype=int)\n",
        "    for i, pos in enumerate(posiciones_info): matriz_hamming[pos] = bits_data[i][1]\n",
        "    for i, pos in enumerate(posiciones_info):\n",
        "        pos_bin = format(pos, '04b')\n",
        "        for j in range(matrix_dimension):\n",
        "            if pos_bin[(matrix_dimension-1) - j] == '1':\n",
        "                bit_activados[j, flags_paridad[j]] = matriz_hamming[pos]\n",
        "                flags_paridad[j] += 1\n",
        "    # for i in range(0, len(matriz_hamming), 4):print(matriz_hamming[i:i+4])\n",
        "    for j in range(matrix_dimension):\n",
        "        for i in bit_activados[j]:\n",
        "            bits_paridad[j] ^= i\n",
        "        matriz_hamming[2 ** j] = bits_paridad[j]\n",
        "    return matriz_hamming\n",
        "def tanda_hamming(batch_size, noise_block_len, block_len, qs, _, name):\n",
        "    ber_q = np.zeros((len(qs), batch_size))\n",
        "    for j, q in enumerate(qs):\n",
        "        data = np.random.randint(0, 2, (batch_size, block_len, 1))\n",
        "        noise = np.random.binomial(1, q, size=(batch_size, noise_block_len, 1)).astype(int)\n",
        "        for i in range(0, batch_size):\n",
        "            raw_data = (data[i:i + 1, :, :]).squeeze().squeeze()\n",
        "            batch_noise = (noise[i:i + 1, :, :]).squeeze().squeeze()\n",
        "            posiciones_info, matrix_dimension = hamming_data(raw_data)\n",
        "            encoded_vector = hamming_encode(raw_data, posiciones_info, matrix_dimension)\n",
        "            channel_vector = canal_bsc(encoded_vector, batch_noise)\n",
        "            decoded_vector = hamming_decode(channel_vector, posiciones_info)\n",
        "            ber = calculate_ber(raw_data, decoded_vector)\n",
        "            ber_q[j][i] = ber\n",
        "    return ber_q\n",
        "def guardar_csv(data, filename):\n",
        "    filepath = filename  # Ajusta la ruta según dónde quieras guardar el archivo\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Q values', 'Average BER'])\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "def tanda_raw(batch_size, block_len, qs, _, name):\n",
        "    ber_q = np.zeros((len(qs), batch_size))\n",
        "    for j, q in enumerate(qs):\n",
        "        data = np.random.randint(0, 2, (batch_size, block_len, 1))\n",
        "        noise = np.random.binomial(1, q, size=(batch_size,block_len, 1)).astype(int)\n",
        "        for i in range(0, batch_size):\n",
        "            raw_data = (data[i:i + 1, :, :]).squeeze().squeeze()\n",
        "            batch_noise = (noise[i:i + 1, :, :]).squeeze().squeeze()\n",
        "            channel_vector = canal_bsc(raw_data, batch_noise)\n",
        "            ber = calculate_ber(raw_data, channel_vector)\n",
        "            ber_q[j][i] = ber\n",
        "    return ber_q\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    block_len = 11\n",
        "    noise_block_len = 16\n",
        "    batch_size = 100\n",
        "    code_rate = 3\n",
        "    num_block = 100000\n",
        "    num_test_batch = int(num_block / batch_size)\n",
        "\n",
        "    qs = np.linspace(0.0001, 0.15, 20)\n",
        "    arrays_q_raw = []\n",
        "    arrays_q_HAM = []\n",
        "    print(f\"Total:\", num_test_batch, \"tandas\")\n",
        "    for _ in range(num_test_batch):\n",
        "        ber_q_HAM = tanda_hamming(batch_size, noise_block_len, block_len, qs, _,\"H1.csv\")\n",
        "        arrays_q_HAM.append(ber_q_HAM)\n",
        "        ber_q_raw = tanda_raw(batch_size, block_len, qs, _, \"RAW1.csv\")\n",
        "        arrays_q_raw.append(ber_q_raw)\n",
        "\n",
        "    qs_reshaped = qs.reshape(20, 1)\n",
        "\n",
        "    HAM_stack = np.hstack(arrays_q_HAM)\n",
        "    HAM_data = np.hstack((qs_reshaped, HAM_stack))\n",
        "    HAM_data_transposed = HAM_data.T\n",
        "    guardar_matriz_en_csv(HAM_data, \"raw50.csv\")\n",
        "\n",
        "    raw_stack = np.hstack(arrays_q_raw)\n",
        "    raw_data = np.hstack((qs_reshaped, raw_stack))\n",
        "    RAW_data_transposed = raw_data.T\n",
        "    guardar_matriz_en_csv(raw_data, \"raw50.csv\")\n",
        "\n",
        "\n",
        "    print(raw_data.shape)\n",
        "    print(HAM_data.shape)\n"
      ],
      "metadata": {
        "id": "YzH4OSa2XDI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Turbo códigos\n",
        "\n",
        "import sys\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "from turbo import TurboEncoder, AWGN, TurboDecoder\n",
        "\n",
        "def canal_bsc(mensaje, ruido):\n",
        "    msg_bits = ((mensaje + 1)/2).astype(int)\n",
        "    msg_canal_bits = np.bitwise_xor(msg_bits, ruido)\n",
        "    msg_canal = (msg_canal_bits * 2) - 1\n",
        "    return msg_canal\n",
        "\n",
        "def calculate_ber(original_bits, decoded_bits):\n",
        "    error_count = sum([x != y for x, y in zip(original_bits, decoded_bits)])\n",
        "    ber = error_count / len(original_bits)\n",
        "    return ber\n",
        "def guardar_matriz_en_csv(matriz, nombre_archivo):\n",
        "    # Convertir la matriz a un DataFrame de pandas\n",
        "    df = pd.DataFrame(matriz.T)  # Transponer para que cada columna corresponda a una probabilidad\n",
        "\n",
        "    # Asignar nombres a las columnas como Probabilidad 1, Probabilidad 2, etc.\n",
        "    df.columns = [f'Probabilidad {i + 1}' for i in range(matriz.shape[0])]\n",
        "\n",
        "    # Guardar el DataFrame en un archivo CSV\n",
        "    df.to_csv(nombre_archivo, index=False)\n",
        "def turbo_encode(message_bits):\n",
        "    block_size = len(message_bits)\n",
        "    interleaver = random.sample(range(0, block_size), block_size)\n",
        "    encoder = TurboEncoder(interleaver)\n",
        "    encoded_vector = encoder.execute(message_bits)\n",
        "    return encoded_vector, interleaver\n",
        "\n",
        "def turbo_decode(received_vector, interleaver):\n",
        "    decoder = TurboDecoder(interleaver)\n",
        "    decoded_vector = decoder.execute(received_vector)\n",
        "    decoded_vector = [int(b > 0.0) for b in decoded_vector]\n",
        "    return decoded_vector\n",
        "\n",
        "def tanda_train(batch_size, block_len, code_rate, qs, _, progreso_tanda):\n",
        "    ber_q = np.zeros((len(qs), batch_size))\n",
        "    for j, q in enumerate(qs):\n",
        "        data = np.random.randint(0, 2, (batch_size, block_len, 1))\n",
        "        noise = np.random.binomial(1, q, size=(batch_size, (block_len*code_rate) + 6, 1)).astype(int)\n",
        "        for i in range(0, batch_size):\n",
        "            progreso_tanda.update(1)\n",
        "            raw_data = (data[i:i+1, :, :]).squeeze().squeeze()\n",
        "            batch_noise = (noise[i:i+1, :, :]).squeeze().squeeze()\n",
        "            encoded_vector, interleaver = turbo_encode(raw_data)\n",
        "            coded_antipodal = (encoded_vector * 2) - 1\n",
        "            channel_vector = canal_bsc(coded_antipodal, batch_noise)\n",
        "            decoded_vector = np.array(turbo_decode(channel_vector, interleaver))\n",
        "            received_data = decoded_vector[:len(raw_data)]\n",
        "            ber = calculate_ber(raw_data, received_data)\n",
        "            ber_q[j][i] = ber\n",
        "    return ber_q\n",
        "\n",
        "def guardar_csv(data, filename):\n",
        "    filepath = filename  # Ajusta la ruta según dónde quieras guardar el archivo\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Q values', 'Average BER'])\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    names = [\"turbo_1_3.csv\"]\n",
        "\n",
        "    for name in names:\n",
        "        block_len = 50\n",
        "        batch_size = 100\n",
        "        code_rate = 3\n",
        "        num_block = 100000\n",
        "        num_test_batch = int(num_block/batch_size)\n",
        "        qs = np.linspace(0.0001, 0.15, 20)\n",
        "        num_epoch_test = 100\n",
        "\n",
        "        arrays_q_turbo = []\n",
        "        print(f\"Total:\", num_epoch_test, \"tandas\")\n",
        "        progreso_tanda = tqdm(total=num_epoch_test*20*batch_size, desc=f\"Progreso GENERAL\")\n",
        "        for _ in range(num_epoch_test):\n",
        "            ber_q_turbo = tanda_train(batch_size, block_len, code_rate, qs, _, progreso_tanda)\n",
        "            arrays_q_turbo.append(ber_q_turbo)\n",
        "            #progreso_tanda.update(1)\n",
        "\n",
        "        qs_reshaped = qs.reshape(20, 1)\n",
        "\n",
        "        turbo_stack = np.hstack(arrays_q_turbo)\n",
        "        turbo_data = np.hstack((qs_reshaped, turbo_stack))\n",
        "        guardar_matriz_en_csv(turbo_data, \"turbo_1_3.csv\")\n",
        "        print(turbo_data.shape)"
      ],
      "metadata": {
        "id": "LzHroZoeZbcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Códigos de repetición\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "\n",
        "\n",
        "def canal_bsc(mensaje, ruido):\n",
        "    msg_canal_bits = np.bitwise_xor(mensaje, ruido)\n",
        "    return msg_canal_bits\n",
        "\n",
        "def calculate_ber(original_bits, decoded_bits):\n",
        "    error_count = sum([x != y for x, y in zip(original_bits, decoded_bits)])\n",
        "    ber = error_count / len(original_bits)\n",
        "    return ber\n",
        "def guardar_matriz_en_csv(matriz, nombre_archivo):\n",
        "    # Convertir la matriz a un DataFrame de pandas\n",
        "    df = pd.DataFrame(matriz.T)  # Transponer para que cada columna corresponda a una probabilidad\n",
        "\n",
        "    # Asignar nombres a las columnas como Probabilidad 1, Probabilidad 2, etc.\n",
        "    df.columns = [f'Probabilidad {i + 1}' for i in range(matriz.shape[0])]\n",
        "\n",
        "    # Guardar el DataFrame en un archivo CSV\n",
        "    df.to_csv(nombre_archivo, index=False)\n",
        "def repetition_encoder(message, repeat_factor):\n",
        "    return np.repeat(message, repeat_factor)\n",
        "def repetition_decoder(received_bits, repeat_factor):\n",
        "    decoded_bits = np.reshape(received_bits, (len(received_bits) // repeat_factor, repeat_factor))\n",
        "    decoded_bits = np.sum(decoded_bits, axis=1) > repeat_factor // 2\n",
        "    return decoded_bits.astype(int)\n",
        "\n",
        "\n",
        "def tanda_train(batch_size, block_len, code_rate, qs, _, progreso_tanda):\n",
        "    ber_q = np.zeros((len(qs), batch_size))\n",
        "    for j, q in enumerate(qs):\n",
        "        data = np.random.randint(0, 2, (batch_size, block_len, 1))\n",
        "        noise = np.random.binomial(1, q, size=(batch_size, (block_len*code_rate) , 1)).astype(int)\n",
        "        for i in range(0, batch_size):\n",
        "            progreso_tanda.update(1)\n",
        "            raw_data = (data[i:i+1, :, :]).squeeze().squeeze()\n",
        "            batch_noise = (noise[i:i+1, :, :]).squeeze().squeeze()\n",
        "            encoded_vector = repetition_encoder(raw_data, 3)\n",
        "            channel_vector = canal_bsc(encoded_vector, batch_noise)\n",
        "            decoded_vector = repetition_decoder(channel_vector, 3)\n",
        "            ber = calculate_ber(raw_data, decoded_vector)\n",
        "            ber_q[j][i] = ber\n",
        "    return ber_q\n",
        "\n",
        "def guardar_csv(data, filename):\n",
        "    filepath = filename  # Ajusta la ruta según dónde quieras guardar el archivo\n",
        "    with open(filepath, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Q values', 'Average BER'])\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    names = [\"repeticion_1_3.csv\"]\n",
        "\n",
        "    for name in names:\n",
        "        block_len = 50\n",
        "        batch_size = 100\n",
        "        code_rate = 3\n",
        "        qs = np.linspace(0.0001, 0.15, 20)\n",
        "        num_epoch_test = 100\n",
        "\n",
        "        arrays_q_x3 = []\n",
        "        print(f\"Total:\", num_epoch_test, \"tandas\")\n",
        "        progreso_tanda = tqdm(total=num_epoch_test*20*batch_size, desc=f\"Progreso GENERAL\")\n",
        "        for _ in range(num_epoch_test):\n",
        "            ber_q_x3 = tanda_train(batch_size, block_len, code_rate, qs, _, progreso_tanda)\n",
        "            arrays_q_x3.append(ber_q_x3)\n",
        "            #progreso_tanda.update(1)\n",
        "\n",
        "        qs_reshaped = qs.reshape(20, 1)\n",
        "\n",
        "        x3_stack = np.hstack(arrays_q_x3)\n",
        "        x3_data = np.hstack((qs_reshaped, x3_stack))\n",
        "        guardar_matriz_en_csv(x3_data, \"repeticion_1_3.csv\")\n",
        "        print(x3_data.shape)"
      ],
      "metadata": {
        "id": "_RM9md7FZie_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}